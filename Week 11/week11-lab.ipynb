{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH5001: Introduction to Computer Programming\n",
    "\n",
    "## Excerpts from the 2018/19 Sample Final Project\n",
    "\n",
    "## \"Least Squares Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background information\n",
    "\n",
    "In many applications, we might be given a set of data points\n",
    "\n",
    "$$(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$$\n",
    "\n",
    "in the $x,y$-plane, and required to determine whether there is a relationship between the $x$ variable and the $y$ variable. \n",
    "For example, a financial analyst might need to determine whether there is a relationship between two stock prices, while a climate scientist might need to know whether temperatures are related to levels of a certain pollutant in the atmosphere.\n",
    "\n",
    "A common way to determine whether two data sets $x_1, x_2, \\ldots, x_n$ and $y_1, y_2, \\ldots, y_n$ are related is to find a function\n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "that 'best fits' the data, so that each $y_i$ is approximately equal to $f(x_i)$, with very small error. \n",
    "The function $f(x)$ usually depends on certain parameters for which we need to solve. \n",
    "For example, to determine whether the $x$- and $y$-variables have a linear relationship, we would use a linear function\n",
    "\n",
    "$$f(x) = a_0 + a_1 x$$\n",
    "\n",
    "and try to find the parameters $a_0$ and $a_1$ that make $f(x)$ 'best fit' the data. \n",
    "This means that we want to find the coefficients $a_0$ and $a_1$ for which the quantity\n",
    "\n",
    "$$SSE = \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "\n",
    "is as small as possible. \n",
    "In other words, we want to minimise the sum of squares of the errors (or \"SSE\") between the actual $y$-values (the $y_i$) and the $y$-values 'predicted' by substituting the corresponding $x_i$ values into the equation $y = f(x)$.\n",
    "\n",
    "This process is called **linear least squares regression**, or simply **linear regression**. \n",
    "More generally, **polynomial (least squares) regression** involves assuming that\n",
    "\n",
    "$$f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_m x^m$$\n",
    "\n",
    "for some integer $m \\geq 2$, and finding the parameters $a_0, a_1, a_2 \\ldots, a_m$ that minimise SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Complete Sample Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Auxiliary code\n",
    "\n",
    "In Parts II-IV (below), you will need to use matrix algebra to solve least squares regression problems. (You learned how to do this in your second-year linear algebra module; if you don't remember the details, you will need to do some revision/reading.) Therefore, you will first need to write some Python functions to perform basic matrix operations.\n",
    "\n",
    "Your code should assume that matrices are represented as nested lists, e.g. the matrices \n",
    "$$\\left[ \\begin{array}{ll} 1&2 \\\\ 3&4 \\\\ 5&6 \\end{array} \\right] \\quad \\text{and} \\quad \n",
    "\\left[ \\begin{array}{l} 7 \\\\ 8 \\end{array} \\right]$$\n",
    "would be input and/or output as the nested lists `[[1, 2], [3, 4], [5, 6]]` and `[[7], [8]]`, respectively.\n",
    "\n",
    "**1. [5 marks]** Write a function `matrix_transpose` which computes the transpose of a matrix. \n",
    "\n",
    "**2. [5 marks]** Write a function `matrix_multiplication` which computes the (matrix) product of two matrices.\n",
    "\n",
    "**3. [10 marks]** Write a function `matrix_system_solver` which solves a system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$ for the vector $\\mathbf{x}$. \n",
    "You may restrict to the case where the (input) matrix $A$ is square, and assume that the system has a unique solution. (These assumptions will be valid when you use your code in Parts II-IV.)\n",
    "\n",
    "Again, you must write your functions 'from scratch', using their mathematical definitions. In the submitted report,\n",
    "you **must not** use any `numpy` functions (e.g. `matmul` for matrix multiplication) which would obviously 'short cut' your code, but you may use basic functions for manipulating lists (e.g. `len`, `append`). \n",
    "(If in doubt about what is allowed, please ask.) Of course, when testing your code, it might be a good idea to compare your result with the one produced by `numpy` functions to detect any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Linear regression\n",
    "\n",
    "**4. [10 marks]** Write a brief review of linear regression, explaining, in particular, how one finds the parameters $a_0$ and $a_1$ described above.\n",
    "\n",
    "**5. [10 marks]** Write a function `least_squares_solver_linear` which calculates the linear regression coefficients for an input data set. The input of your function must be a list of the form `[[x1, y1], [x2, y2], ... , [xn, yn]]`, to represent a set of data points $(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$. The output should be a list of the form `[[a0], [a1]]` containing the corresponding coefficients $a_0$ and $a_1$ (in that order).\n",
    "\n",
    "**6. [5 marks]** Run the function `data_linear` from the file \"data.py\". The input must be your student ID, enterred as a string, e.g. `data1('abc123')`; the output will be a set of $n$ data points, in the form of a list as described in question 5. Use your function from question 5 to calculate the linear regression coefficients $a_0$ and $a_1$ for this data, and display the data and the line of best fit on a single, well-labelled plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Quadratic regression\n",
    "\n",
    "**7. [10 marks]** Explain how to extend linear regression to quadratic regression (i.e. to the case $m=2$).\n",
    "\n",
    "**8. [10 marks]** Write a function `least_squares_solver_quadratic` which calculates the quadratic regression coefficients for an input data set. The input should have the same form as in question 5. The output should be a list of the form `[[a0], [a1], [a2]]` containing the corresponding coefficients $a_0$, $a_1$ and $a_2$ (in that order).\n",
    "\n",
    "**9. [5 marks]** Run the function `data_quadratic` from the file \"data.py\". The input must be your student ID, enterred as a string, e.g. `data1('abc123')`; the output will be a set of $n$ data points, in the form of a list as described in question 5. Use your function from question 8 to calculate the quadratic regression coefficients $a_0$, $a_1$ and $a_2$ for this data, and display the data and the best-fit quadratic polynomial on a single, well-labelled plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: General polynomial regression\n",
    "\n",
    "**10. [10 marks]** Explain how to extend linear and quadratic regression to general polynomial regression.\n",
    "\n",
    "**11. [10 marks]** Write a function `least_squares_solver_polynomial` which takes as input a data set (in the same form as in question 5) and the degree $m$ of a polynomial (a positive integer), and computes the corresponding polynomial regression coefficients $a_0, a_1, a_2, \\ldots, a_m$. The output of your function should be a list of the form `[[a0], [a1], [a2], ..., [am]]` containing the $m+1$ coefficients $a_0, a_1, a_2, \\ldots, a_m$ in that order.\n",
    "\n",
    "**12. [10 marks]** Run the function `data_polynomial` from the file \"data.py\". The input must be your student ID, enterred as a string, e.g. `data1('abc123')`; the output will be a set of $n$ data points, in the form of a list as described in question 5. Use your function from question 11 for $m=3,4,5,6$ to calculate the regression coefficients $a_0, a_1, \\cdots, a_m$ for this data, and display the data and the best-fit polynomials on four well-labelled plot.\n",
    "Decide which fit is the best and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Week's Exercises\n",
    "\n",
    "### Below is a partially completed sample solution. We'll provide much of the work, but you should look below for the answers that we left blank and try to complete them. These are questions 5, 8, and 11 (worth 30 of 100 marks).\n",
    "\n",
    "\n",
    "\n",
    "### Part I: Auxiliary code\n",
    "\n",
    "**1. [5 marks]** Write a function `matrix_transpose` which computes the transpose of a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_transpose(A):\n",
    "    \"Returns the transpose of the input matrix A.\"\n",
    "    # calculate the number of rows and columns in A\n",
    "    rows_in_A = len(A)\n",
    "    cols_in_A = len(A[0])\n",
    "    # initialise a list to represent A transpose\n",
    "    AT = []\n",
    "    # A transpose has cols_of_A rows: fill them in row by row\n",
    "    for i in range(cols_in_A):\n",
    "        AT.append([A[j][i] for j in range(rows_in_A)])\n",
    "    return AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0]\n",
      "[3.0, 4.0]\n",
      "[5.0, 6.0]\n",
      "\n",
      "[1.0, 3.0, 5.0]\n",
      "[2.0, 4.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "# test an example\n",
    "matrix=[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n",
    "print(*matrix, sep='\\n')\n",
    "print()\n",
    "print(*matrix_transpose(matrix), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. [5 marks]** Write a function `matrix_multiplication` which computes the (matrix) product of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_multiplication(A, B):\n",
    "    \"Returns the matrix product of the input matrices.\"\n",
    "    # calculate the number of rows and columns in AB\n",
    "    # AB has the same number of rows as A, and the same number of columns as B\n",
    "    rows = len(A)\n",
    "    cols = len(B[0])\n",
    "    # initialise a nested list of length rows to represent the matrix product AB\n",
    "    AB = [[] for i in range(rows)]\n",
    "    # calculate the number of rows in B (equivalently, the number of columns in A)\n",
    "    m = len(B)\n",
    "    # fill in each row of AB using the definition of matrix multiplication\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # fill in entry (i,j) of AB\n",
    "            AB[i].append(sum([A[i][k]*B[k][j] for k in range(m)]))\n",
    "    return AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0]\n",
      "[0.0, 1.0]\n",
      "[1.0, 0.0]\n",
      "\n",
      "[1.0, 0.0, -1.0]\n",
      "[0.0, 1.0, 0.0]\n",
      "\n",
      "[1.0, 0.0, -1.0]\n",
      "[0.0, 1.0, 0.0]\n",
      "[1.0, 0.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "# test an example\n",
    "matrix1=[[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
    "matrix2=[[1.0, 0.0, -1.0], [0.0, 1.0, 0.0]]\n",
    "print(*matrix1, sep='\\n')\n",
    "print()\n",
    "print(*matrix2, sep='\\n')\n",
    "print()\n",
    "print(*matrix_multiplication(matrix1,matrix2), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. [10 marks]** Write a function `matrix_system_solver` which solves a system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$ for the vector $\\mathbf{x}$. \n",
    "You may restrict to the case where the (input) matrix $A$ is square, and assume that the system has a unique solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_system_solver(A, b): \n",
    "    \n",
    "    \"\"\"\n",
    "    Solves the system of linear equations Ax=b for x.\n",
    "    Code adapted from:\n",
    "    https://martin-thoma.com/solving-linear-equations-with-gaussian-elimination/\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the number of rows and columns in A (assuming A is square)\n",
    "    n = len(A)\n",
    "    \n",
    "    # construct the augmented matrix M=[A|b]\n",
    "    # intialise a nested list to represent M\n",
    "    M = [[] for i in range(n)]\n",
    "    # fill in one row at a time\n",
    "    for i in range(n):\n",
    "        # fill in each entry in the current row\n",
    "        for j in range(n):\n",
    "            # fill in the (i,j) entry of A\n",
    "            M[i].append(A[i][j])\n",
    "        # put coordinate i of the vector b in the final column of M\n",
    "        M[i].append(b[i][0])\n",
    "\n",
    "    # begin Gaussian elimination\n",
    "    # look at the columns of M one by one\n",
    "    for i in range(n):\n",
    "        # find the largest absolute value element in the \n",
    "        # current column (column i) in row i or below\n",
    "        max_element = abs(M[i][i])\n",
    "        max_row = i\n",
    "        for k in range(i+1, n):\n",
    "            if abs(M[k][i]) > max_element:\n",
    "                max_element = abs(M[k][i])\n",
    "                max_row = k\n",
    "        # swap the 'maximum' row with row i\n",
    "        for k in range(i, n+1):\n",
    "            element_to_swap = M[max_row][k]\n",
    "            M[max_row][k] = M[i][k]\n",
    "            M[i][k] = element_to_swap\n",
    "        # apply row operations to make all entries below row i in column i zero\n",
    "        for k in range(i+1, n):\n",
    "            # define the constant term in the row operation\n",
    "            constant = -M[k][i]/M[i][i]\n",
    "            # apply the row operation \"R_k -> R_k+constant*R_i\" column by column \n",
    "            for j in range(i, n+1):\n",
    "                if j == i:\n",
    "                    M[k][j] = 0\n",
    "                else:\n",
    "                    M[k][j] = M[k][j] + (constant * M[i][j])\n",
    "    # end Gaussian elimination\n",
    "\n",
    "    # begin back substitution\n",
    "    # initialise a nested list to represent the solution vector x\n",
    "    x = [[] for i in range(n)]\n",
    "    # start at row n of the augmented matrix M and work up\n",
    "    for i in range(n-1, -1, -1):\n",
    "        # fill in coordinate i of x\n",
    "        x[i].append(M[i][n]/M[i][i])\n",
    "        # update 'b' values before next step of outer loop so that\n",
    "        # x[i] will be correctly computed via the above line of code \n",
    "        # on the next pass\n",
    "        for k in range(i-1, -1, -1):\n",
    "            M[k][n] = M[k][n] - (M[k][i] * x[i][0])\n",
    "    # end back substitution\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0]\n",
      "[1.5, 1.0]\n",
      "\n",
      "[2.0]\n",
      "[1.0]\n",
      "\n",
      "[-2.0]\n",
      "[4.0]\n",
      "\n",
      "[2.0]\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "# test an example\n",
    "example_A,example_b = [[1.0, 1.0], [1.5, 1.0]],[[2.0], [1.0]]\n",
    "example_x = matrix_system_solver(example_A, example_b)\n",
    "example_Ax = matrix_multiplication(example_A, example_x)\n",
    "print(*example_A,sep='\\n')\n",
    "print()\n",
    "print(*example_b,sep='\\n')\n",
    "print()\n",
    "print(*example_x,sep='\\n')\n",
    "print()\n",
    "print(*example_Ax,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Linear regression\n",
    "\n",
    "**4. [10 marks]** Write a brief review of linear regression, explaining, in particular, how one finds the parameters $a_0$ and $a_1$ described above.\n",
    "\n",
    "We use the notation introduced in the \"Background information\" section above. \n",
    "Given the data points $(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$, consider the matrices\n",
    "\n",
    "$$X = \\left[ \\begin{array}{ll} 1&x_0 \\\\ 1&x_1 \\\\ \\vdots&\\vdots \\\\ 1&x_n \\end{array} \\right], \\quad\n",
    "\\mathbf{y} = \\left[ \\begin{array}{l} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right], \\quad \\text{and} \\quad\n",
    "\\mathbf{a} = \\left[ \\begin{array}{l} a_0 \\\\ a_1 \\end{array} \\right].$$\n",
    "\n",
    "Then, by definition of matrix multiplication, we have\n",
    "\n",
    "$$SSE = ||\\mathbf{y} - X\\mathbf{a}||^2,$$\n",
    "\n",
    "where $||\\cdot||$ is the usual Euclidean norm of a vector. \n",
    "We need to choose the vector $\\mathbf{a}$ such that SSE is minimised. \n",
    "In other words, $\\mathbf{a}$ should be a *least squares solution* of the system of linear equations\n",
    "\n",
    "$$X\\mathbf{a} = \\mathbf{y}.$$\n",
    "\n",
    "By using the theory of orthogonal projections from linear algebra, it can be shown that $\\mathbf{a}$ must therefore be an (actual) solution of the corresponding *normal equations*, i.e. the system of linear equations\n",
    "\n",
    "$$(X^TX)\\mathbf{a} = X^T\\mathbf{y}.$$\n",
    "\n",
    "A detailed proof may be found in, e.g. Theorem 7.33 on p.101 of the following linear algebra notes:\n",
    "\n",
    "https://qmplus.qmul.ac.uk/mod/resource/view.php?id=1367999\n",
    "\n",
    "In summary, starting with our data set, we should form the matrices $X$ and $\\mathbf{y}$ as above, and solve the system of equations $(X^TX)\\mathbf{a} = X^T\\mathbf{y}$ for the vector $\\mathbf{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. [10 marks]** Write a function `least_squares_solver_linear` which calculates the linear regression coefficients for an input data set. The input of your function must be a list of the form `[[x1, y1], [x2, y2], ... , [xn, yn]]`, to represent a set of data points $(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$. The output should be a list of the form `[[a0], [a1]]` containing the corresponding coefficients $a_0$ and $a_1$ (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. [5 marks]** Run the function `data_linear` from the file \"data.py\". The input must be your student ID, enterred as a string, e.g. `data1('abc123')`; the output will be a set of $n$ data points, in the form of a list as described in question 5. Use your function from question 5 to calculate the linear regression coefficients $a_0$ and $a_1$ for this data, and display the data and the line of best fit on a single, well-labelled plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "# sort the data by x value (smallest to largest) to make plotting easier\n",
    "data_linear = sorted(data.data_linear('abc123'))\n",
    "sol_linear = least_squares_solver_linear(data_linear)\n",
    "a0 = sol_linear[0][0]\n",
    "a1 = sol_linear[1][0]\n",
    "print('a0 =', a0)\n",
    "print('a1 =', a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEXCAYAAABF40RQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nSoWg/wfov+7GKK8KOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# construct lists of x and y values from the data\n",
    "xi_linear = [data_linear[i][0] for i in range(len(data_linear))]\n",
    "yi_linear = [data_linear[i][1] for i in range(len(data_linear))]\n",
    "# plot the (x,y) data\n",
    "plt.scatter(xi_linear, yi_linear, label='sample data')\n",
    "# construct a list of predicted y values using the line of best fit\n",
    "yi_predicted_linear = [a0 + a1*xi_linear[i] for i in range(len(data_linear))]\n",
    "# plot the line of best fit\n",
    "plt.plot(xi_linear, yi_predicted_linear, color='red', label='line of best fit')\n",
    "# add title/labels and show plot\n",
    "plt.title('Linear sample data and line of best fit')\n",
    "plt.xlabel('x_i')\n",
    "plt.ylabel('y_i')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Quadratic regression\n",
    "\n",
    "**7. [10 marks]** Explain how to extend linear regression to quadratic regression (i.e. to the case $m=2$).\n",
    "\n",
    "Here we again use the notation introduced in the \"Background information\" section above. \n",
    "We now wish to fit a function\n",
    "\n",
    "$$f(x) = a_0 + a_1x + a_2x^2$$\n",
    "\n",
    "to our data points $(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$, in such a way that\n",
    "\n",
    "$$SSE = ||y_i - f(x_i)||^2$$\n",
    "\n",
    "is minimised.\n",
    "If we now consider the matrices\n",
    "\n",
    "$$X = \\left[ \\begin{array}{ll} 1&x_0&x_0^2 \\\\ 1&x_1&x_1^2 \\\\ \\vdots&\\vdots \\\\ 1&x_n&x_n^2 \\end{array} \\right], \\quad\n",
    "\\mathbf{y} = \\left[ \\begin{array}{l} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right], \\quad \\text{and} \\quad\n",
    "\\mathbf{a} = \\left[ \\begin{array}{l} a_0 \\\\ a_1 \\\\ a_2 \\end{array} \\right],$$\n",
    "\n",
    "then we again find that\n",
    "\n",
    "$$SSE = ||\\mathbf{y} - X\\mathbf{a}||^2.$$\n",
    "\n",
    "Therefore, $\\mathbf{a}$ must again be chosen to be a least squares solution of the system\n",
    "\n",
    "$$X\\mathbf{a} = \\mathbf{y},$$\n",
    "\n",
    "and so we can again calculate $\\mathbf{a}$ as a solution of the normal equations\n",
    "\n",
    "$$(X^TX)\\mathbf{a} = X^T\\mathbf{y}.$$\n",
    "\n",
    "This again follows from Theorem 7.33 on p.101 of the following linear algebra notes:\n",
    "\n",
    "https://qmplus.qmul.ac.uk/mod/resource/view.php?id=1367999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. [10 marks]** Write a function `least_squares_solver_quadratic` which calculates the quadratic regression coefficients for an input data set. The input should have the same form as in question 5. The output should be a list of the form `[[a0], [a1], [a2]]` containing the corresponding coefficients $a_0$, $a_1$ and $a_2$ (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. [5 marks]** Run the function `data_quadratic` from the file \"data.py\". The input must be your student ID, enterred as a string, e.g. `data1('abc123')`; the output will be a set of $n$ data points, in the form of a list as described in question 5. Use your function from question 8 to calculate the quadratic regression coefficients $a_0$, $a_1$ and $a_2$ for this data, and display the data and the best-fit quadratic polynomial on a single, well-labelled plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "# sort the data by x value to make plotting easier\n",
    "data_quadratic = sorted(data.data_quadratic('abc123'))\n",
    "sol_quadratic = least_squares_solver_quadratic(data_quadratic)\n",
    "a0 = sol_quadratic[0][0]\n",
    "a1 = sol_quadratic[1][0]\n",
    "a2 = sol_quadratic[2][0]\n",
    "print('a0 =', a0)\n",
    "print('a1 =', a1)\n",
    "print('a2 =', a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# construct lists of x and y values from the data\n",
    "xi_quadratic = [data_quadratic[i][0] for i in range(len(data_quadratic))]\n",
    "yi_quadratic = [data_quadratic[i][1] for i in range(len(data_quadratic))]\n",
    "# plot the (x,y) data\n",
    "plt.scatter(xi_quadratic, yi_quadratic, label='sample data')\n",
    "# construct a list of predicted y values using the quadrtic of best fit\n",
    "yi_predicted_quadratic = [a0 + a1*xi_quadratic[i] + a2*xi_quadratic[i]**2 for i in range(len(data_quadratic))]\n",
    "# plot the quadratic of best fit\n",
    "plt.plot(xi_quadratic, yi_predicted_quadratic, color='red', label='best-fit quadratic')\n",
    "# add title/labels and show plot\n",
    "plt.title('Quadratic sample data and best-fit quadratic polynomial')\n",
    "plt.xlabel('x_i')\n",
    "plt.ylabel('y_i')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: General polynomial regression\n",
    "\n",
    "**10. [10 marks]** Explain how to extend linear and quadratic regression to general polynomial regression.\n",
    "\n",
    "In the general case, we wish to fit a function\n",
    "\n",
    "$$f(x) = a_0 + a_1x + a_2x^2 + \\cdots a_mx^m$$\n",
    "\n",
    "to our data points $(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)$, where $m$ is some (chosen but arbitrarily large) positive integer. \n",
    "Observe that $m=1$ for linear regression, and $m=2$ for quadratic regression. \n",
    "As in those previous cases, we once again need to minimise\n",
    "\n",
    "$$SSE = ||y_i - f(x_i)||^2.$$\n",
    "\n",
    "We now consider the matrices\n",
    "\n",
    "$$X = \\left[ \\begin{array}{ll} 1&x_0&x_0^2&\\cdots&x_0^m \\\\ 1&x_1&x_1^2&\\cdots&x_1^m \\\\ \\vdots&\\vdots \\\\ 1&x_n&x_n^2&\\cdots&x_n^m \\end{array} \\right], \\quad\n",
    "\\mathbf{y} = \\left[ \\begin{array}{l} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right], \\quad \\text{and} \\quad\n",
    "\\mathbf{a} = \\left[ \\begin{array}{l} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right],$$\n",
    "\n",
    "and we again find that\n",
    "\n",
    "$$SSE = ||\\mathbf{y} - X\\mathbf{a}||^2,$$\n",
    "\n",
    "meaning that $\\mathbf{a}$ should again be chosen to be a least squares solution of the system $X\\mathbf{a} = \\mathbf{y}$. \n",
    "Hence, we can once again find $\\mathbf{a}$ by solving the normal equations\n",
    "\n",
    "$$(X^TX)\\mathbf{a} = X^T\\mathbf{y}.$$\n",
    "\n",
    "As before, this follows from, e.g. Theorem 7.33 on p.101 of the following linear algebra notes:\n",
    "\n",
    "https://qmplus.qmul.ac.uk/mod/resource/view.php?id=1367999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. [10 marks]** Write a function `least_squares_solver_polynomial` which takes as input a data set (in the same form as in question 5) and the degree $m$ of a polynomial (a positive integer), and computes the corresponding polynomial regression coefficients $a_0, a_1, a_2, \\ldots, a_m$. The output of your function should be a list of the form `[[a0], [a1], [a2], ..., [am]]` containing the $m+1$ coefficients $a_0, a_1, a_2, \\ldots, a_m$ in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_solver_polynomial(data, m):\n",
    "    \"\"\"\n",
    "    Returns a list [a0, a1, a2, ..., am] of coefficients of best fit polynomial\n",
    "    y = a0 + a1*x + a2*x**2 + ... + am*x**m for general polynomial least squares regression.\n",
    "    \"\"\"\n",
    "    # calculate the number of data points\n",
    "    n = len(data)\n",
    "    # construct the matrix X column by column\n",
    "    # column 1 contains all 1's\n",
    "    X = [[1] for i in range(n)]\n",
    "    # construct the other columns\n",
    "    for j in range(1,m+1):\n",
    "        for i in range(n):\n",
    "            X[i].append(data[i][0]**j)\n",
    "    # construct the matrix (vector) y\n",
    "    y = [[data[i][1]] for i in range(n)]\n",
    "    # construct the transpose X, X^TX and X^Ty\n",
    "    XT = matrix_transpose(X)\n",
    "    XTX = matrix_multiplication(XT, X)\n",
    "    XTy = matrix_multiplication(XT, y)\n",
    "    # solve X^TXa = X^Ty for the vector a = (a0, a1, a2, ..., am)^T\n",
    "    a = matrix_system_solver(XTX, XTy) \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. [10 marks]** Run the function `data_polynomial` from the file \"data.py\". The input must be your student ID, enterred as a string, e.g. `data1('abc123')`; the output will be a set of $n$ data points, in the form of a list as described in question 5. Use your function from question 11 for $m=3,4,5,6$ to calculate the regression coefficients $a_0, a_1, \\cdots, a_m$ for this data, and display the data and the best-fit polynomials on four well-labelled plot.\n",
    "Decide which fit is the best and explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import data\n",
    "import matplotlib.pyplot as plt\n",
    "# sort the data by x value to make plotting easier\n",
    "data_polynomial = sorted(data.data_polynomial('abc123'))\n",
    "for m in range(2,7):\n",
    "    sol_polynomial = least_squares_solver_polynomial(data_polynomial, m)\n",
    "    sol=matrix_transpose(sol_polynomial)[0]\n",
    "    print(\"Fit polynomial of degree\", m)\n",
    "    print(*zip(range(len(sol)),sol),sep='\\n')\n",
    "    # construct lists of x and y values from the data\n",
    "    xi_polynomial = [data_polynomial[i][0] for i in range(len(data_polynomial))]\n",
    "    yi_polynomial = [data_polynomial[i][1] for i in range(len(data_polynomial))]\n",
    "    # plot the (x,y) data\n",
    "    plt.scatter(xi_polynomial, yi_polynomial, label='sample data')\n",
    "    # construct a list of predicted y values using the quintic of best fit\n",
    "    yi_predicted_polynomial = [sum(\n",
    "        [sol[j]*xi_polynomial[i]**j for j in range(m+1)]\n",
    "    ) for i in range(len(data_polynomial))]\n",
    "    # plot the quintic of best fit\n",
    "    plt.plot(xi_polynomial, yi_predicted_polynomial, color='red', label='best-fit')\n",
    "    # add title/labels and show plot\n",
    "    plt.title('Polynomial sample data and best-fit polynomial')\n",
    "    plt.xlabel('x_i')\n",
    "    plt.ylabel('y_i')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
